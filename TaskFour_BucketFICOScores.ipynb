{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1739c1d",
   "metadata": {},
   "source": [
    "# Task Four\n",
    "\n",
    "Charlie wants to make her model work for future data sets, so she needs a general approach to generating the buckets. Given a set number of buckets corresponding to the number of input labels for the model, she would like to find out the boundaries that best summarize the data. You need to create a rating map that maps the FICO score of the borrowers to a rating where a lower rating signifies a better credit score.\n",
    "\n",
    "The process of doing this is known as quantization. You could consider many ways of solving the problem by optimizing different properties of the resulting buckets, such as the mean squared error or log-likelihood (see below for definitions).\n",
    "\n",
    "Mean squared error\n",
    "\n",
    "You can view this question as an approximation problem and try to map all the entries in a bucket to one value, minimizing the associated squared error. We are now looking to minimize the following:\n",
    "\n",
    "$$ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} ( Y_i - \\hat{Y}_i)^2 $$\n",
    "\n",
    "Log-likelihood\n",
    "\n",
    "A more sophisticated possibility is to maximize the following log-likelihood function:\n",
    "\n",
    "$$ LL(b_1,\\ldots, b_{r-1}) = \\sum_{i=1}^{r}[k_i \\ln{p_i} + (n_i - k_i) \\ln{(1 -p_i)}] $$\n",
    "\n",
    "Where $b_i$ is the bucket boundaries, $n_i$ is the number of records in each bucket, $k_i$ is the number of defaults in each bucket, and $p_i = k_i / n_i$  is the probability of default in the bucket. This function considers how rough the discretization is and the density of defaults in each bucket. This problem could be addressed by splitting it into subproblems, which can be solved incrementally (i.e., through a dynamic programming approach). For example, you can break the problem into two subproblems, creating five buckets for FICO scores ranging from 0 to 600 and five buckets for FICO scores ranging from 600 to 850."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee9ed9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the above mentioned tasks to quantify FICO scores into seperate buckets\n",
    "\n",
    "# Import the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81b87b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the loan data and extract the FICO scores and default columns\n",
    "loan_data = pd.read_csv('Loan_Data.csv')\n",
    "fico_scores = loan_data['fico_score']\n",
    "default_status = loan_data['default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa606cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following the discussion above, we will employ different methods to bucket the FICO scores\n",
    "# Method 1: Mean squared error minimization\n",
    "def mse_bucketing(fico_scores, default_status, num_buckets):\n",
    "    # Define the range of FICO scores\n",
    "    min_fico = fico_scores.min()\n",
    "    max_fico = fico_scores.max()\n",
    "    \n",
    "    # Initialize bucket boundaries\n",
    "    bucket_boundaries = np.linspace(min_fico, max_fico, num_buckets + 1)\n",
    "    \n",
    "    # Function to calculate MSE for given bucket boundaries\n",
    "    def calculate_mse(boundaries):\n",
    "        mse = 0\n",
    "        for i in range(len(boundaries) - 1):\n",
    "            bucket_mask = (fico_scores >= boundaries[i]) & (fico_scores < boundaries[i + 1])\n",
    "            bucket_defaults = default_status[bucket_mask]\n",
    "            if len(bucket_defaults) > 0:\n",
    "                default_rate = bucket_defaults.mean()\n",
    "                mse += ((default_rate - default_status.mean()) ** 2) * len(bucket_defaults)\n",
    "        return mse / len(fico_scores)\n",
    "    \n",
    "    # Optimize bucket boundaries to minimize MSE\n",
    "    result = minimize(calculate_mse, bucket_boundaries, method='L-BFGS-B')\n",
    "    \n",
    "    optimized_boundaries = result.x\n",
    "    return optimized_boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "422ebe20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Log-likelihood maximization\n",
    "def log_likelihood_bucketing(fico_scores, default_status, num_buckets):\n",
    "    # Define the range of FICO scores\n",
    "    min_fico = fico_scores.min()\n",
    "    max_fico = fico_scores.max()\n",
    "    \n",
    "    # Initialize bucket boundaries\n",
    "    bucket_boundaries = np.linspace(min_fico, max_fico, num_buckets + 1)\n",
    "    \n",
    "    # Function to calculate negative log-likelihood for given bucket boundaries\n",
    "    def calculate_neg_log_likelihood(boundaries):\n",
    "        neg_log_likelihood = 0\n",
    "        for i in range(len(boundaries) - 1):\n",
    "            bucket_mask = (fico_scores >= boundaries[i]) & (fico_scores < boundaries[i + 1])\n",
    "            bucket_defaults = default_status[bucket_mask]\n",
    "            if len(bucket_defaults) > 0:\n",
    "                default_rate = bucket_defaults.mean()\n",
    "                # Avoid log(0) by adding a small constant\n",
    "                default_rate = np.clip(default_rate, 1e-5, 1 - 1e-5)\n",
    "                neg_log_likelihood -= np.sum(bucket_defaults * np.log(default_rate) + (1 - bucket_defaults) * np.log(1 - default_rate))\n",
    "        return neg_log_likelihood\n",
    "    \n",
    "    # Optimize bucket boundaries to maximize log-likelihood (minimize negative log-likelihood)\n",
    "    result = minimize(calculate_neg_log_likelihood, bucket_boundaries, method='L-BFGS-B')\n",
    "    \n",
    "    optimized_boundaries = result.x\n",
    "    return optimized_boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aae16022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: Dynamic programming approach\n",
    "\n",
    "def dp_bucketing_nll_prebinned(\n",
    "    fico_scores,\n",
    "    default_status,\n",
    "    num_buckets,\n",
    "    num_prebins=100\n",
    "):\n",
    "    # Step 1: Sort data\n",
    "    sorted_idx = np.argsort(fico_scores)\n",
    "    fico = np.asarray(fico_scores)[sorted_idx]\n",
    "    y = np.asarray(default_status)[sorted_idx]\n",
    "\n",
    "    n = len(fico)\n",
    "\n",
    "    # Step 2: Pre-binning (quantile-based)\n",
    "    bin_edges = np.linspace(0, n, num_prebins + 1, dtype=int)\n",
    "\n",
    "    prebinned_counts = []\n",
    "    prebinned_defaults = []\n",
    "    prebinned_fico = []\n",
    "\n",
    "    for i in range(num_prebins):\n",
    "        start, end = bin_edges[i], bin_edges[i + 1]\n",
    "        if start == end:\n",
    "            continue\n",
    "\n",
    "        cnt = end - start\n",
    "        dsum = y[start:end].sum()\n",
    "\n",
    "        prebinned_counts.append(cnt)\n",
    "        prebinned_defaults.append(dsum)\n",
    "        prebinned_fico.append(fico[start])\n",
    "\n",
    "    prebinned_counts = np.array(prebinned_counts)\n",
    "    prebinned_defaults = np.array(prebinned_defaults)\n",
    "    prebinned_fico = np.array(prebinned_fico)\n",
    "\n",
    "    m = len(prebinned_counts)\n",
    "\n",
    "    # Step 3: DP tables\n",
    "    dp = np.full((m + 1, num_buckets + 1), np.inf)\n",
    "    split = np.zeros((m + 1, num_buckets + 1), dtype=int)\n",
    "    dp[0, 0] = 0.0\n",
    "\n",
    "    eps = 1e-8\n",
    "\n",
    "    # Step 4: DP recurrence with NLL cost\n",
    "    for j in range(1, num_buckets + 1):\n",
    "        for i in range(j, m + 1):\n",
    "            best_cost = dp[i, j]\n",
    "\n",
    "            for k in range(j - 1, i):\n",
    "                total_count = prebinned_counts[k:i].sum()\n",
    "                total_defaults = prebinned_defaults[k:i].sum()\n",
    "\n",
    "                if total_count == 0:\n",
    "                    continue\n",
    "\n",
    "                p = np.clip(total_defaults / total_count, eps, 1 - eps)\n",
    "\n",
    "                # Bernoulli negative log-likelihood\n",
    "                nll = (\n",
    "                    - total_defaults * np.log(p)\n",
    "                    - (total_count - total_defaults) * np.log(1 - p)\n",
    "                )\n",
    "\n",
    "                cost = dp[k, j - 1] + nll\n",
    "\n",
    "                if cost < best_cost:\n",
    "                    best_cost = cost\n",
    "                    split[i, j] = k\n",
    "\n",
    "            dp[i, j] = best_cost\n",
    "\n",
    "    # Step 5: Backtrack boundaries\n",
    "    boundaries = [prebinned_fico[0]]\n",
    "    idx = m\n",
    "\n",
    "    for j in range(num_buckets, 0, -1):\n",
    "        idx = split[idx, j]\n",
    "        boundaries.append(prebinned_fico[idx])\n",
    "\n",
    "    return sorted(set(boundaries))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93d97a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Bucketing Boundaries: [408.  496.4 584.8 673.2 761.6 850. ]\n"
     ]
    }
   ],
   "source": [
    "# Compare the results from the three methods\n",
    "num_buckets = 5\n",
    "mse_boundaries = mse_bucketing(fico_scores, default_status, num_buckets)\n",
    "print(\"MSE Bucketing Boundaries:\", mse_boundaries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36ff93b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-Likelihood Bucketing Boundaries: [408.00000556 496.4        584.8        673.2        761.6\n",
      " 849.99999991]\n"
     ]
    }
   ],
   "source": [
    "ll_boundaries = log_likelihood_bucketing(fico_scores, default_status, num_buckets)\n",
    "print(\"Log-Likelihood Bucketing Boundaries:\", ll_boundaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e7e292d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamic Programming Bucketing Boundaries: [np.int64(408), np.int64(553), np.int64(611), np.int64(649), np.int64(720)]\n"
     ]
    }
   ],
   "source": [
    "dp_boundaries = dp_bucketing_nll_prebinned(fico_scores, default_status, num_buckets, num_prebins=500)\n",
    "print(\"Dynamic Programming Bucketing Boundaries:\", dp_boundaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd20b6d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
